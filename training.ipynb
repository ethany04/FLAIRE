{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Loading the Data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:39.450247Z","iopub.status.busy":"2023-05-08T06:46:39.449766Z","iopub.status.idle":"2023-05-08T06:46:39.457761Z","shell.execute_reply":"2023-05-08T06:46:39.45628Z","shell.execute_reply.started":"2023-05-08T06:46:39.450207Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","\n","transform = transforms.Compose([transforms.Resize((64, 64)),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n","                               ])"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:40.213855Z","iopub.status.busy":"2023-05-08T06:46:40.213439Z","iopub.status.idle":"2023-05-08T06:46:40.230643Z","shell.execute_reply":"2023-05-08T06:46:40.228995Z","shell.execute_reply.started":"2023-05-08T06:46:40.213823Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/train'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/Skele/CONVERGENT/flaire github/f23-bt-ticketing/training.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Skele/CONVERGENT/flaire%20github/f23-bt-ticketing/training.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata/train\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransform)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    310\u001b[0m         root,\n\u001b[1;32m    311\u001b[0m         loader,\n\u001b[1;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"]}],"source":["training_dataset = datasets.ImageFolder(root='data/train', transform=transform)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:40.922499Z","iopub.status.busy":"2023-05-08T06:46:40.921822Z","iopub.status.idle":"2023-05-08T06:46:40.929843Z","shell.execute_reply":"2023-05-08T06:46:40.928338Z","shell.execute_reply.started":"2023-05-08T06:46:40.922458Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['Accident', 'Non Accident']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["training_dataset.classes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:41.571215Z","iopub.status.busy":"2023-05-08T06:46:41.570853Z","iopub.status.idle":"2023-05-08T06:46:41.582656Z","shell.execute_reply":"2023-05-08T06:46:41.581765Z","shell.execute_reply.started":"2023-05-08T06:46:41.571187Z"},"trusted":true},"outputs":[],"source":["validation_dataset = datasets.ImageFolder(root='data/val', transform=transform)\n","test_dataset = datasets.ImageFolder(root='data/test', transform=transform)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:42.223875Z","iopub.status.busy":"2023-05-08T06:46:42.222872Z","iopub.status.idle":"2023-05-08T06:46:42.230013Z","shell.execute_reply":"2023-05-08T06:46:42.228647Z","shell.execute_reply.started":"2023-05-08T06:46:42.223833Z"},"trusted":true},"outputs":[],"source":["batch_size = 64\n","train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:42.87232Z","iopub.status.busy":"2023-05-08T06:46:42.871887Z","iopub.status.idle":"2023-05-08T06:46:43.84282Z","shell.execute_reply":"2023-05-08T06:46:43.841549Z","shell.execute_reply.started":"2023-05-08T06:46:42.872256Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 3, 64, 64]) torch.Size([64])\n"]}],"source":["batch = next(iter(train_loader))\n","images,labels = batch\n","print(images.shape, labels.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualizing Images"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:46.127215Z","iopub.status.busy":"2023-05-08T06:46:46.12642Z","iopub.status.idle":"2023-05-08T06:46:46.768982Z","shell.execute_reply":"2023-05-08T06:46:46.768018Z","shell.execute_reply.started":"2023-05-08T06:46:46.127176Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/Skele/CONVERGENT/flaire github/f23-bt-ticketing/training.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Skele/CONVERGENT/flaire%20github/f23-bt-ticketing/training.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m6\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Skele/CONVERGENT/flaire%20github/f23-bt-ticketing/training.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Skele/CONVERGENT/flaire%20github/f23-bt-ticketing/training.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(images[i][\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Skele/CONVERGENT/flaire%20github/f23-bt-ticketing/training.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}],"source":["for i in range(6):\n","    plt.subplot(2, 3, i + 1)\n","    plt.imshow(images[i][0])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Convolutional Neural Network"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:50.464958Z","iopub.status.busy":"2023-05-08T06:46:50.464379Z","iopub.status.idle":"2023-05-08T06:46:50.472382Z","shell.execute_reply":"2023-05-08T06:46:50.471183Z","shell.execute_reply.started":"2023-05-08T06:46:50.464924Z"},"trusted":true},"outputs":[],"source":["class CNNModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),  \n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # output: 32 x 16 x 16\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # output: 64 x 8 x 8\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # output: 128 x 4 x 4\n","            nn.Flatten(),\n","            nn.Linear(128*4*4, 1024),  # Updated linear layer input size\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 2)\n","        )\n","\n","    def forward(self, X):\n","        return self.network(X)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T06:46:53.527762Z","iopub.status.busy":"2023-05-08T06:46:53.527373Z","iopub.status.idle":"2023-05-08T06:49:50.515934Z","shell.execute_reply":"2023-05-08T06:49:50.514759Z","shell.execute_reply.started":"2023-05-08T06:46:53.527733Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0], Model Accuracy: 0.6406\n","Epoch [1], Model Accuracy: 0.6392\n","Epoch [2], Model Accuracy: 0.5731\n","Epoch [3], Model Accuracy: 0.6025\n","Epoch [4], Model Accuracy: 0.7583\n","Epoch [5], Model Accuracy: 0.8249\n","Epoch [6], Model Accuracy: 0.8318\n","Epoch [7], Model Accuracy: 0.8346\n","Epoch [8], Model Accuracy: 0.8952\n","Epoch [9], Model Accuracy: 0.8571\n","Epoch [10], Model Accuracy: 0.8943\n","Epoch [11], Model Accuracy: 0.8934\n","Epoch [12], Model Accuracy: 0.8814\n","Epoch [13], Model Accuracy: 0.9040\n","Epoch [14], Model Accuracy: 0.9177\n","Epoch [15], Model Accuracy: 0.9099\n","Epoch [16], Model Accuracy: 0.8961\n","Epoch [17], Model Accuracy: 0.8874\n","Epoch [18], Model Accuracy: 0.8511\n","Epoch [19], Model Accuracy: 0.8952\n"]}],"source":["model = CNNModel()\n","loss = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n","num_epochs = 20\n","\n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","\n","for epoch in range(num_epochs):\n","    for batch in train_loader:\n","        #forward pass\n","        inputs, labels = batch\n","        outputs = model(inputs)\n","        l = loss(outputs, labels)\n","        #backward pass\n","        l.backward()\n","        #weight update\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    \n","    batch_acc = []\n","    for batch in val_loader:\n","        inputs, labels = batch\n","        outputs = model(inputs)\n","        acc = accuracy(outputs, labels)\n","        batch_acc.append(acc)\n","    epoch_acc = torch.stack(batch_acc).mean()\n","    print(f\"Epoch [{epoch}], Model Accuracy: {epoch_acc.item():.4f}\")"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy of Model is 0.9470\n","Processed 100 images in 1.185 seconds, 84.38 FPS\n"]}],"source":["import time\n","\n","# Define the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Send the model to the defined device\n","model.to(device)\n","\n","# Initialize a list to store accuracies for each batch\n","batch_acc = []\n","\n","# Start timing\n","start_time = time.time()\n","\n","# Loop over all batches in the test loader\n","for inputs, labels in test_loader:\n","    # Send data to the device\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    \n","    # No need to track gradients for inference\n","    with torch.no_grad():\n","        outputs = model(inputs)\n","    \n","    # Calculate accuracy and store it\n","    acc = accuracy(outputs, labels)\n","    batch_acc.append(acc)\n","\n","# End timing\n","end_time = time.time()\n","\n","# Calculate the mean accuracy across all batches\n","test_accuracy = torch.stack(batch_acc).mean()\n","print(f\"Test accuracy of Model is {test_accuracy:.4f}\")\n","\n","# Calculate the elapsed time for processing\n","elapsed_time = end_time - start_time\n","\n","# Calculate the total number of images processed\n","total_images = len(test_loader.dataset)\n","\n","# Calculate frames per second (FPS)\n","fps = total_images / elapsed_time\n","print(f\"Processed {total_images} images in {elapsed_time:.3f} seconds, {fps:.2f} FPS\")\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["test_accuracy_float = float(f\"{test_accuracy:.4f}\")\n","if test_accuracy_float > 0.9427:\n","    torch.save(model, 'model.pth')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the model on the test images: 92.0%\n","Frame 0: True Label - 0, Predicted Label - 0\n","Frame 1: True Label - 0, Predicted Label - 0\n","Frame 2: True Label - 0, Predicted Label - 0\n","Frame 3: True Label - 0, Predicted Label - 0\n","Frame 4: True Label - 0, Predicted Label - 0\n","Frame 5: True Label - 0, Predicted Label - 0\n","Frame 6: True Label - 0, Predicted Label - 0\n","Frame 7: True Label - 0, Predicted Label - 0\n","Frame 8: True Label - 0, Predicted Label - 0\n","Frame 9: True Label - 0, Predicted Label - 0\n","Frame 10: True Label - 0, Predicted Label - 0\n","Frame 11: True Label - 0, Predicted Label - 0\n","Frame 12: True Label - 0, Predicted Label - 0\n","Frame 13: True Label - 0, Predicted Label - 0\n","Frame 14: True Label - 0, Predicted Label - 0\n","Frame 15: True Label - 0, Predicted Label - 0\n","Frame 16: True Label - 0, Predicted Label - 0\n","Frame 17: True Label - 0, Predicted Label - 0\n","Frame 18: True Label - 0, Predicted Label - 0\n","Frame 19: True Label - 0, Predicted Label - 0\n","Frame 20: True Label - 0, Predicted Label - 0\n","Frame 21: True Label - 0, Predicted Label - 0\n","Frame 22: True Label - 0, Predicted Label - 0\n","Frame 23: True Label - 0, Predicted Label - 1\n","Frame 24: True Label - 0, Predicted Label - 0\n","Frame 25: True Label - 0, Predicted Label - 0\n","Frame 26: True Label - 0, Predicted Label - 0\n","Frame 27: True Label - 0, Predicted Label - 0\n","Frame 28: True Label - 0, Predicted Label - 0\n","Frame 29: True Label - 0, Predicted Label - 0\n","Frame 30: True Label - 0, Predicted Label - 0\n","Frame 31: True Label - 0, Predicted Label - 0\n","Frame 32: True Label - 0, Predicted Label - 1\n","Frame 33: True Label - 0, Predicted Label - 0\n","Frame 34: True Label - 0, Predicted Label - 0\n","Frame 35: True Label - 0, Predicted Label - 0\n","Frame 36: True Label - 0, Predicted Label - 0\n","Frame 37: True Label - 0, Predicted Label - 1\n","Frame 38: True Label - 0, Predicted Label - 1\n","Frame 39: True Label - 0, Predicted Label - 0\n","Frame 40: True Label - 0, Predicted Label - 0\n","Frame 41: True Label - 0, Predicted Label - 1\n","Frame 42: True Label - 0, Predicted Label - 1\n","Frame 43: True Label - 0, Predicted Label - 0\n","Frame 44: True Label - 0, Predicted Label - 0\n","Frame 45: True Label - 0, Predicted Label - 1\n","Frame 46: True Label - 0, Predicted Label - 0\n","Frame 47: True Label - 1, Predicted Label - 1\n","Frame 48: True Label - 1, Predicted Label - 1\n","Frame 49: True Label - 1, Predicted Label - 1\n","Frame 50: True Label - 1, Predicted Label - 1\n","Frame 51: True Label - 1, Predicted Label - 1\n","Frame 52: True Label - 1, Predicted Label - 1\n","Frame 53: True Label - 1, Predicted Label - 1\n","Frame 54: True Label - 1, Predicted Label - 1\n","Frame 55: True Label - 1, Predicted Label - 1\n","Frame 56: True Label - 1, Predicted Label - 1\n","Frame 57: True Label - 1, Predicted Label - 1\n","Frame 58: True Label - 1, Predicted Label - 1\n","Frame 59: True Label - 1, Predicted Label - 1\n","Frame 60: True Label - 1, Predicted Label - 1\n","Frame 61: True Label - 1, Predicted Label - 1\n","Frame 62: True Label - 1, Predicted Label - 1\n","Frame 63: True Label - 1, Predicted Label - 1\n","Frame 64: True Label - 1, Predicted Label - 1\n","Frame 65: True Label - 1, Predicted Label - 1\n","Frame 66: True Label - 1, Predicted Label - 1\n","Frame 67: True Label - 1, Predicted Label - 1\n","Frame 68: True Label - 1, Predicted Label - 1\n","Frame 69: True Label - 1, Predicted Label - 1\n","Frame 70: True Label - 1, Predicted Label - 1\n","Frame 71: True Label - 1, Predicted Label - 1\n","Frame 72: True Label - 1, Predicted Label - 1\n","Frame 73: True Label - 1, Predicted Label - 1\n","Frame 74: True Label - 1, Predicted Label - 1\n","Frame 75: True Label - 1, Predicted Label - 1\n","Frame 76: True Label - 1, Predicted Label - 1\n","Frame 77: True Label - 1, Predicted Label - 1\n","Frame 78: True Label - 1, Predicted Label - 1\n","Frame 79: True Label - 1, Predicted Label - 1\n","Frame 80: True Label - 1, Predicted Label - 1\n","Frame 81: True Label - 1, Predicted Label - 1\n","Frame 82: True Label - 1, Predicted Label - 1\n","Frame 83: True Label - 1, Predicted Label - 1\n","Frame 84: True Label - 1, Predicted Label - 1\n","Frame 85: True Label - 1, Predicted Label - 1\n","Frame 86: True Label - 1, Predicted Label - 1\n","Frame 87: True Label - 1, Predicted Label - 1\n","Frame 88: True Label - 1, Predicted Label - 1\n","Frame 89: True Label - 1, Predicted Label - 0\n","Frame 90: True Label - 1, Predicted Label - 1\n","Frame 91: True Label - 1, Predicted Label - 1\n","Frame 92: True Label - 1, Predicted Label - 1\n","Frame 93: True Label - 1, Predicted Label - 1\n","Frame 94: True Label - 1, Predicted Label - 1\n","Frame 95: True Label - 1, Predicted Label - 1\n","Frame 96: True Label - 1, Predicted Label - 1\n","Frame 97: True Label - 1, Predicted Label - 1\n","Frame 98: True Label - 1, Predicted Label - 1\n","Frame 99: True Label - 1, Predicted Label - 1\n"]}],"source":["saved_model = torch.load('0.95.pth')\n","saved_model.eval()\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():  # Disable gradient tracking\n","    for inputs, labels in test_loader:\n","        outputs = saved_model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the predicted classes\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f'Accuracy of the model on the test images: {accuracy}%')\n","\n","import torch\n","\n","# Loop through the test dataset\n","for i, (inputs, labels) in enumerate(test_loader):\n","    outputs = saved_model(inputs)\n","    _, predicted = torch.max(outputs, 1)\n","\n","    for j in range(inputs.size(0)):  # Iterate over each image in the batch\n","        image = inputs[j]  # Get the image\n","        true_label = labels[j].item()  # Actual label\n","        pred_label = predicted[j].item()  # Predicted label\n","\n","        print(f\"Frame {i * test_loader.batch_size + j}: True Label - {true_label}, Predicted Label - {pred_label}\")\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":4}
